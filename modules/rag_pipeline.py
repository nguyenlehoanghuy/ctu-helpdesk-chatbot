import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from config import MODEL_PATH, INDEX_PATH, LOG_FILE
from modules.retriever import RetrievalSystem
import logging
import traceback

# -----------------------
# Logging setup
# -----------------------
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding="utf-8"),
        logging.StreamHandler()
    ]
)


class CTUChatbot:
    """
    Chatbot pipeline gá»“m 2 pháº§n:
    - Retriever: RetrievalSystem (FAISS + BM25 + TF-IDF hybrid search)
    - Generator: LLM model (AutoModelForCausalLM)
    """
    def __init__(self, model_path=MODEL_PATH, index_path=INDEX_PATH):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ğŸš€ Initializing CTUChatbot (device: {self.device})")

        # --- Load retriever ---
        self.retrieval = RetrievalSystem()
        try:
            self.retrieval.load_index(index_path)
            logger.info(f"âœ… Loaded retrieval index from: {index_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to load retrieval index: {e}")
            raise

        # --- Load model ---
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                dtype=torch.bfloat16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            logger.info(f"âœ… Model loaded from: {model_path}")
        except Exception as e:
            logger.error(f"âŒ Failed to load model: {e}")
            raise

        logger.info("ğŸ’¡ CTUChatbot ready to serve.")

    def answer_query(self, query: str) -> str:
        """
        Nháº­n cÃ¢u há»i, retrieve context, sinh cÃ¢u tráº£ lá»i ngáº¯n gá»n.
        """
        try:
            if not query.strip():
                return "Xin hÃ£y nháº­p cÃ¢u há»i há»£p lá»‡."

            # Retrieve top documents
            results = self.retrieval.search(query, top_k=1, method='hybrid')
            if not results:
                logger.warning(f"âš ï¸ No retrieval results for query: {query}")
                context = "(KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u phÃ¹ há»£p.)"
            else:
                # Náº¿u cÃ³ parent_id, láº¥y ná»™i dung gá»‘c, trÃ¡nh máº¥t ngá»¯ cáº£nh
                contexts = []
                for r in results:
                    doc = r.get("document", {})
                    parent_id = doc.get("parent_id", doc.get("id"))
                    full_doc = next((d for d in self.retrieval.documents if d["id"] == parent_id), None)
                    if full_doc:
                        contexts.append(full_doc.get("original_content", full_doc.get("content", "")))
                    else:
                        contexts.append(r.get("content", ""))
                context = "\n\n".join(contexts)

            # Build prompt
            prompt = f"Dá»±a vÃ o thÃ´ng tin sau Ä‘Ã¢y vá» há»‡ thá»‘ng CTU Helpdesk:\n\n{context}\n\nHÃ£y tráº£ lá»i cÃ¢u há»i: {query}"

            messages = [
                {"role": "system", "content": "Báº¡n lÃ  trá»£ lÃ½ AI cho há»‡ thá»‘ng Helpdesk CTU."},
                {"role": "user", "content": prompt}
            ]

            # Tokenize & Generate
            # Náº¿u tokenizer chÆ°a há»— trá»£ apply_chat_template thÃ¬ cÃ³ thá»ƒ táº¡o prompt thá»§ cÃ´ng
            if hasattr(self.tokenizer, "apply_chat_template"):
                text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            else:
                # Fallback: ná»‘i chuá»—i Ä‘Æ¡n giáº£n
                text = f"{messages[0]['content']}\n{messages[1]['content']}"

            inputs = self.tokenizer([text], return_tensors="pt", padding=True).to(self.device)

            outputs = self.model.generate(
                inputs.input_ids,
                attention_mask=inputs.attention_mask,
                max_new_tokens=512,
                do_sample=False,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id
            )

            # Láº¥y pháº§n output generated (loáº¡i bá» pháº§n prompt input)
            new_tokens = outputs[0][inputs.input_ids.shape[-1]:]
            answer = self.tokenizer.decode(new_tokens, skip_special_tokens=True).strip()
            return answer

        except Exception as e:
            logger.error(f"âŒ Error while generating answer: {e}")
            logger.debug(traceback.format_exc())
            return "Xin lá»—i, tÃ´i gáº·p lá»—i khi xá»­ lÃ½ cÃ¢u há»i."


# -----------------------
# Global instance handler
# -----------------------
_chatbot_instance = None


def handle_user_message(user_message: str) -> str:
    """
    HÃ m gá»i nhanh (dÃ¹ng cho webhook Flask Messenger)
    - Giá»¯ instance model + retriever trong RAM (singleton)
    """
    global _chatbot_instance

    try:
        if _chatbot_instance is None:
            _chatbot_instance = CTUChatbot()
            logger.info("ğŸŒŸ New chatbot instance created in memory.")

        return _chatbot_instance.answer_query(user_message)

    except Exception as e:
        logger.error(f"âŒ Error in handle_user_message: {e}")
        return "Xin lá»—i, há»‡ thá»‘ng Ä‘ang gáº·p sá»± cá»‘. Vui lÃ²ng thá»­ láº¡i sau."
